
variable "my_secret_value" {
  description = "Secret retrieved from Azure Key Vault"
  type        = string
  sensitive   = true
}


resource "azurerm_resource_group" "example" {
  name     = "example-resources"
  location = "East US"

  tags = {
    secret_value = var.my_secret_value  # Use the variable
  }
}


provider "azurerm" {
  features {}
}


trigger:
- main

pool:
  vmImage: 'ubuntu-latest'

variables:
  - group: MyVariableGroup  # Optional: Variable group for non-secret variables

stages:
- stage: Terraform_Deploy
  displayName: 'Terraform Deployment'
  jobs:
  - job: Terraform
    steps:

    - task: AzureKeyVault@2
      displayName: 'Retrieve Secrets from Key Vault'
      inputs:
        azureSubscription: 'MyAzureServiceConnection'  # Service connection name
        KeyVaultName: 'my-keyvault-name'  # Azure Key Vault name
        SecretsFilter: '*'  # Fetch all secrets

    - script: |
        echo "Initializing Terraform..."
        terraform init
      displayName: 'Terraform Init'

    - script: |
        echo "Applying Terraform Configuration..."
        terraform apply -auto-approve
      displayName: 'Terraform Apply'
      env:
        TF_VAR_my_secret_value: $(my-secret-key)  # Pass secret securely







Introduction "Today, I'd like to walk you through an automated solution we’ve implemented to streamline our log management processes in the Terraform deployment pipeline. This solution allows us to append filtered log data efficiently, ensuring relevant details are saved for review while keeping the process straightforward and reproducible."
Key Details of the Solution

Automated Log Filtering and Appending

We've developed a Python script that captures log output from Terraform plans. The script filters out irrelevant or known information like "known after apply" entries, focusing on significant updates.
Using Python’s file operations in append mode, this solution ensures that every Terraform
plan run adds only the filtered information to a single log file. This helps maintain an organized log history without overwriting valuable data.
Enhanced Efficiency and Readability

By filtering logs, we’ve reduced the volume of information to only the essential details, making it much easier to track updates, changes, or issues.
The solution is implemented in a way that aligns with our pipeline’s existing infrastructure, meaning it
integrates seamlessly with other stages of the deployment.
Reliability and Scalability

This approach is designed to be both reliable and scalable, capable of handling multiple runs without risk of data loss. It ensures we maintain a clean, readable log file, which will support troubleshooting and audit needs as deployments increase in complexity.
Conclusion and Benefits

This solution demonstrates our commitment to efficient, transparent, and organized deployment practices. It minimizes manual intervention, reduces error potential, and ensures that critical information remains accessible. With these improvements, we’re better positioned to support current and future deployment needs smoothly."
